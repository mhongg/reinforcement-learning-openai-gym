{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MountainCar_base (new size).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBofT6FUE2nO"
      },
      "source": [
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1PKWBc9E4PC"
      },
      "source": [
        "!pip install box2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pc3RHlQE5y-"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "__author__ = 'zhenhang.sun@gmail.com'\n",
        "__version__ = '1.0.0'\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 256)\n",
        "        self.linear2 = nn.Linear(256, 128)\n",
        "        self.linear3 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.eval_net = Net(self.state_space_dim, self.action_space_dim).to(self.device)\n",
        "        self.target_net = Net(self.state_space_dim, self.action_space_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=self.lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.epsi = [self.epsi_high] * 200\n",
        "        self.curr_epsi = self.epsi_high\n",
        "        self.buffer = []\n",
        "        self.steps = 0\n",
        "        self.episodes = 0\n",
        "        \n",
        "    def act(self, s0):\n",
        "        if random.random() < self.curr_epsi:\n",
        "            # Select next action randomly\n",
        "            a0 = random.randrange(self.action_space_dim)\n",
        "        else:\n",
        "            # Select next action according to policy\n",
        "            self.eval_net.eval()\n",
        "            with torch.no_grad():\n",
        "                s0 = torch.FloatTensor(s0).view(1,-1).to(self.device)\n",
        "                a0 = torch.argmax(self.eval_net(s0)).item()\n",
        "        \n",
        "        self.steps += 1\n",
        "        return a0\n",
        "\n",
        "    def put(self, s0, a0, r1, s1, done):\n",
        "        # Save transition to replay buffer\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((s0, a0, r1, s1, done))\n",
        "\n",
        "        if done:\n",
        "            self.episodes += 1\n",
        "\n",
        "    def sample(self):\n",
        "        error_samples = random.sample( self.buffer, self.batch_size)\n",
        "\n",
        "        s0, a0, r1, s1, d1 = zip(*error_samples)\n",
        "        s0 = torch.FloatTensor(s0).to(self.device)\n",
        "        a0 = torch.LongTensor(a0).view(self.batch_size, -1).to(self.device)\n",
        "        r1 = torch.FloatTensor(r1).view(self.batch_size, -1).to(self.device)\n",
        "        s1 = torch.FloatTensor(s1).to(self.device)\n",
        "        d1 = torch.FloatTensor(d1).view(self.batch_size, -1).to(self.device)\n",
        "\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = self.eval_net(s0).gather(1, a0)\n",
        "            y_true = r1 + (1-d1) * self.gamma * torch.max(self.target_net(s1).detach(), dim=1)[0].view(self.batch_size, -1)\n",
        "\n",
        "        errors = abs(y_pred - y_true)\n",
        "        indices = torch.sort(errors, dim=0, descending=True)[1]\n",
        "        indices = torch.flatten(indices).tolist()\n",
        "        indices = indices[:int(self.batch_size/2)]\n",
        "\n",
        "        if self.batch_size % 2 == 0:\n",
        "            samples = random.sample(self.buffer, int(self.batch_size/2))\n",
        "        else:\n",
        "            samples = random.sample(self.buffer, int(self.batch_size/2)+1)\n",
        "        # samples = random.sample(self.buffer, int(self.batch_size/2)+int(self.batch_size%2==1))\n",
        "\n",
        "        for idx in indices:\n",
        "            samples.append(error_samples[idx])\n",
        "\n",
        "        # Fix overlap transitions\n",
        "\n",
        "        return samples\n",
        "\n",
        "        \n",
        "    def learn(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample a batch of transitions from replay buffer\n",
        "        #--- uncomment for prioritized replay ---#\n",
        "        # samples = self.sample()\n",
        "        #----------------------------------------#\n",
        "\n",
        "        #--- uncomment for normal replay ---#\n",
        "        samples = random.sample( self.buffer, self.batch_size)\n",
        "        #-----------------------------------#\n",
        "        \n",
        "        s0, a0, r1, s1, d1 = zip(*samples)\n",
        "        s0 = torch.FloatTensor(s0).to(self.device)\n",
        "        a0 = torch.LongTensor(a0).view(self.batch_size, -1).to(self.device)\n",
        "        r1 = torch.FloatTensor(r1).view(self.batch_size, -1).to(self.device)\n",
        "        s1 = torch.FloatTensor(s1).to(self.device)\n",
        "        d1 = torch.FloatTensor(d1).view(self.batch_size, -1).to(self.device)\n",
        "\n",
        "        # Compute current and target Q-values\n",
        "        self.eval_net.train()\n",
        "        y_pred = self.eval_net(s0).gather(1, a0)\n",
        "        y_true = r1 + (1-d1) * self.gamma * torch.max(self.target_net(s1).detach(), dim=1)[0].view(self.batch_size, -1)\n",
        "        \n",
        "        # Optimize model\n",
        "        loss = self.loss_fn(y_pred, y_true)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_episode(self, episode_rewards):\n",
        "        # Increase batch size\n",
        "        #--- uncomment for adaptive batch size ---#\n",
        "        # mean = np.mean(episode_rewards[-30:])\n",
        "        # if len(episode_rewards) >= 30 and mean > -150:\n",
        "        #     self.batch_size = min(356, self.batch_size+1)\n",
        "        # else:\n",
        "        #     self.batch_size = min(256, self.batch_size)\n",
        "        #-----------------------------------------#\n",
        "\n",
        "        # Update epsilons\n",
        "        #--- uncomment for adaptive epsilon ---#\n",
        "        # for n in range(len(self.epsi)):\n",
        "        #     self.epsi_low =  0.00 + ((0.01*n)/200)\n",
        "        #     self.decay = 0.99 - ((0.01*(200-n))/200)\n",
        "        #     self.epsi[n] = max(self.epsi_low, self.epsi[n] * self.decay)\n",
        "        # self.curr_epsi = self.epsi[self.steps-1]\n",
        "        #--------------------------------------#\n",
        "\n",
        "        #--- uncomment for normal epsilon ---#\n",
        "        self.curr_epsi = max(self.epsi_low, self.curr_epsi * self.decay)\n",
        "        #------------------------------------#\n",
        "        \n",
        "        self.steps = 0\n",
        "\n",
        "        # Update target network\n",
        "        if self.episodes % self.update_interval == 0:\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "            self.target_net.eval()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHn9DPhuFGS7"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "__author__ = 'zhenhang.sun@gmail.com'\n",
        "__version__ = '1.0.0'\n",
        "\n",
        "# cd MountainCar-v0\n",
        "# activate gym\n",
        "# python mountaincar-0.py\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from dqn_mountaincar import Agent\n",
        "\n",
        "\n",
        "def plot_rewards():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    rewards_t = torch.FloatTensor(episode_rewards)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Rewards')\n",
        "    plt.plot(rewards_t.numpy())\n",
        "    # take 100 episode averages and plot them too\n",
        "    if len(rewards_t) >= 100:\n",
        "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.cat(99*[torch.tensor([-200])]), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if len(rewards_t) >= 100 and means[-1:] >= -110:\n",
        "        plt.title('Solved!')\n",
        "        plt.pause(0.001)\n",
        "        return True\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    trial_episodes = []\n",
        "    for trial in range(10):\n",
        "        env = gym.make('MountainCar-v0')\n",
        "\n",
        "        params = {\n",
        "            'gamma': 1,\n",
        "            'epsi_high': 0.5,\n",
        "            'epsi_low': 0.01,\n",
        "            'decay': 0.99,  #0.95 (0.97, 0.98 not work)\n",
        "            'lr': 0.001, #0.0005\n",
        "            'capacity': 400000,\n",
        "            'batch_size': 256,\n",
        "            'update_interval': 1,   # unit: episode\n",
        "            'state_space_dim': env.observation_space.shape[0],\n",
        "            'action_space_dim': env.action_space.n\n",
        "        }\n",
        "        agent = Agent(**params)\n",
        "\n",
        "        episode_rewards = []\n",
        "        for episode in range(2000):\n",
        "            s0 = env.reset()\n",
        "            tot_rewards = 0\n",
        "\n",
        "            while True:\n",
        "                # env.render()\n",
        "                a0 = agent.act(s0)\n",
        "                s1, r1, done, _ = env.step(a0)      \n",
        "        \n",
        "                agent.put(s0, a0, r1, s1, done)\n",
        "                tot_rewards += r1 \n",
        "                \n",
        "                if done:\n",
        "                    episode_rewards.append(tot_rewards)\n",
        "                    success = plot_rewards()\n",
        "                    break\n",
        "\n",
        "                s0 = s1\n",
        "                agent.learn()\n",
        "\n",
        "            agent.update_episode(episode_rewards)\n",
        "\n",
        "            print(\"Episode: {} \\tRewards: {} \\tBatchSize: {} \\tEpsi: {}\".format(episode, tot_rewards, agent.batch_size, agent.curr_epsi))\n",
        "            if success:\n",
        "                print(\"Solved!\")\n",
        "                break\n",
        "        trial_episodes.append(episode)\n",
        "        print(trial_episodes)\n",
        "\n",
        "        env.close()\n",
        "        plt.ioff()\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47xopO1IFR41"
      },
      "source": [
        "import numpy as np \n",
        "print(trial_episodes)\n",
        "print(np.mean(trial_episodes)) # [450, 715]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}