{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LunarLander_base.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOCXWqF89od-"
      },
      "source": [
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBxwVvGN9q5n"
      },
      "source": [
        "!pip install box2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsw6Oq9v9sO_"
      },
      "source": [
        "# coding: utf-8\r\n",
        "\r\n",
        "__author__ = 'zhenhang.sun@gmail.com'\r\n",
        "__version__ = '1.0.0'\r\n",
        "\r\n",
        "import gym\r\n",
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size):\r\n",
        "        super().__init__()\r\n",
        "        self.linear1 = nn.Linear(input_size, 256)\r\n",
        "        self.linear2 = nn.Linear(256, 128)\r\n",
        "        self.linear3 = nn.Linear(128, output_size)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.linear1(x))\r\n",
        "        x = F.relu(self.linear2(x))\r\n",
        "        x = self.linear3(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "class Agent(object):\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        for key, value in kwargs.items():\r\n",
        "            setattr(self, key, value)\r\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "        self.eval_net = Net(self.state_space_dim, self.action_space_dim).to(self.device)\r\n",
        "        self.target_net = Net(self.state_space_dim, self.action_space_dim).to(self.device)\r\n",
        "        self.target_net.load_state_dict(self.eval_net.state_dict())\r\n",
        "        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=self.lr)\r\n",
        "        self.loss_fn = nn.MSELoss()\r\n",
        "        self.epsi = [self.epsi_high] * 1000\r\n",
        "        self.curr_epsi = self.epsi_high\r\n",
        "        self.buffer = []\r\n",
        "        self.steps = 0\r\n",
        "        self.episodes = 0\r\n",
        "        \r\n",
        "    def act(self, s0):\r\n",
        "        if random.random() < self.curr_epsi:\r\n",
        "            # Select next action randomly\r\n",
        "            a0 = random.randrange(self.action_space_dim)\r\n",
        "        else:\r\n",
        "            # Select next action according to policy\r\n",
        "            self.eval_net.eval()\r\n",
        "            with torch.no_grad():\r\n",
        "                s0 = torch.FloatTensor(s0).view(1,-1).to(self.device)\r\n",
        "                a0 = torch.argmax(self.eval_net(s0)).item()\r\n",
        "        \r\n",
        "        self.steps += 1\r\n",
        "        return a0\r\n",
        "\r\n",
        "    def put(self, s0, a0, r1, s1, done):\r\n",
        "        # Save transition to replay buffer\r\n",
        "        if len(self.buffer) == self.capacity:\r\n",
        "            self.buffer.pop(0)\r\n",
        "        self.buffer.append((s0, a0, r1, s1, done))\r\n",
        "\r\n",
        "        if done:\r\n",
        "            self.episodes += 1\r\n",
        "\r\n",
        "    def sample(self):\r\n",
        "        error_samples = random.sample( self.buffer, self.batch_size)\r\n",
        "\r\n",
        "        s0, a0, r1, s1, d1 = zip(*error_samples)\r\n",
        "        s0 = torch.FloatTensor(s0).to(self.device)\r\n",
        "        a0 = torch.LongTensor(a0).view(self.batch_size, -1).to(self.device)\r\n",
        "        r1 = torch.FloatTensor(r1).view(self.batch_size, -1).to(self.device)\r\n",
        "        s1 = torch.FloatTensor(s1).to(self.device)\r\n",
        "        d1 = torch.FloatTensor(d1).view(self.batch_size, -1).to(self.device)\r\n",
        "\r\n",
        "        self.eval_net.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            y_pred = self.eval_net(s0).gather(1, a0)\r\n",
        "            y_true = r1 + (1-d1) * self.gamma * torch.max(self.target_net(s1).detach(), dim=1)[0].view(self.batch_size, -1)\r\n",
        "\r\n",
        "        errors = abs(y_pred - y_true)\r\n",
        "        indices = torch.sort(errors, dim=0, descending=True)[1]\r\n",
        "        indices = torch.flatten(indices).tolist()\r\n",
        "        indices = indices[:int(self.batch_size/2)]\r\n",
        "\r\n",
        "        if self.batch_size % 2 == 0:\r\n",
        "            samples = random.sample(self.buffer, int(self.batch_size/2))\r\n",
        "        else:\r\n",
        "            samples = random.sample(self.buffer, int(self.batch_size/2)+1)\r\n",
        "        # samples = random.sample(self.buffer, int(self.batch_size/2)+int(self.batch_size%2==1))\r\n",
        "\r\n",
        "        for idx in indices:\r\n",
        "            samples.append(error_samples[idx])\r\n",
        "\r\n",
        "        # Fix overlap transitions\r\n",
        "\r\n",
        "        return samples\r\n",
        "\r\n",
        "        \r\n",
        "    def learn(self):\r\n",
        "        if len(self.buffer) < self.batch_size:\r\n",
        "            return\r\n",
        "        \r\n",
        "        # Sample a batch of transitions from replay buffer\r\n",
        "        #--- uncomment for prioritized replay ---#\r\n",
        "        # samples = self.sample()\r\n",
        "        #----------------------------------------#\r\n",
        "\r\n",
        "        #--- uncomment for normal replay ---#\r\n",
        "        samples = random.sample( self.buffer, self.batch_size)\r\n",
        "        #-----------------------------------#\r\n",
        "        \r\n",
        "        s0, a0, r1, s1, d1 = zip(*samples)\r\n",
        "        s0 = torch.FloatTensor(s0).to(self.device)\r\n",
        "        a0 = torch.LongTensor(a0).view(self.batch_size, -1).to(self.device)\r\n",
        "        r1 = torch.FloatTensor(r1).view(self.batch_size, -1).to(self.device)\r\n",
        "        s1 = torch.FloatTensor(s1).to(self.device)\r\n",
        "        d1 = torch.FloatTensor(d1).view(self.batch_size, -1).to(self.device)\r\n",
        "\r\n",
        "        # Compute current and target Q-values\r\n",
        "        self.eval_net.train()\r\n",
        "        y_pred = self.eval_net(s0).gather(1, a0)\r\n",
        "        y_true = r1 + (1-d1) * self.gamma * torch.max(self.target_net(s1).detach(), dim=1)[0].view(self.batch_size, -1)\r\n",
        "        \r\n",
        "        # Optimize model\r\n",
        "        loss = self.loss_fn(y_pred, y_true)\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "    def update_episode(self, episode_rewards):\r\n",
        "        # Increase batch size\r\n",
        "        #--- uncomment for adaptive batch size ---#\r\n",
        "        # mean = np.mean(episode_rewards[-50:])\r\n",
        "        # if len(episode_rewards) >= 50 and mean > 170:\r\n",
        "        #     self.batch_size = min(256, self.batch_size+1)\r\n",
        "        # else:\r\n",
        "        #     self.batch_size = min(128, self.batch_size)\r\n",
        "        #-----------------------------------------#\r\n",
        "\r\n",
        "        # Update epsilons\r\n",
        "        #--- uncomment for adaptive epsilon ---#\r\n",
        "        # for n in range(len(self.epsi)):\r\n",
        "        #     self.epsi_low =  (0.01/1000) * n\r\n",
        "        #     self.decay = 0.995 - (0.01*(1000-n))/1000\r\n",
        "        #     self.epsi[n] = max(self.epsi_low, self.epsi[n] * self.decay)    \r\n",
        "        # self.curr_epsi = self.epsi[self.steps-1]\r\n",
        "        #--------------------------------------#\r\n",
        "\r\n",
        "        #--- uncomment for normal epsilon ---#\r\n",
        "        self.curr_epsi = max(self.epsi_low, self.curr_epsi * self.decay)\r\n",
        "        #------------------------------------#\r\n",
        "        \r\n",
        "        self.steps = 0\r\n",
        "\r\n",
        "        # Update target network\r\n",
        "        if self.episodes % self.update_interval == 0:\r\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\r\n",
        "            self.target_net.eval()\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gLC57IN9tXC"
      },
      "source": [
        "# coding: utf-8\r\n",
        "\r\n",
        "__author__ = 'zhenhang.sun@gmail.com'\r\n",
        "__version__ = '1.0.0'\r\n",
        "\r\n",
        "# cd LunarLander-v2\r\n",
        "# activate gym\r\n",
        "# python lunarlander-0.py\r\n",
        "\r\n",
        "import gym\r\n",
        "import torch\r\n",
        "from IPython import display\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# from dqn_lunarlander import Agent\r\n",
        "\r\n",
        "\r\n",
        "def plot_rewards():\r\n",
        "    plt.figure(2)\r\n",
        "    plt.clf()\r\n",
        "    rewards_t = torch.FloatTensor(episode_rewards)\r\n",
        "    plt.title('Training...')\r\n",
        "    plt.xlabel('Episode')\r\n",
        "    plt.ylabel('Total Rewards')\r\n",
        "    plt.plot(rewards_t.numpy())\r\n",
        "    # take 100 episode averages and plot them too\r\n",
        "    if len(rewards_t) >= 100:\r\n",
        "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\r\n",
        "        means = torch.cat((torch.cat(99*[torch.tensor([-200])]), means))\r\n",
        "        plt.plot(means.numpy())\r\n",
        "\r\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\r\n",
        "    if len(rewards_t) >= 100 and means[-1:] >= 200:\r\n",
        "        plt.title('Solved!')\r\n",
        "        plt.pause(0.001)\r\n",
        "        return True\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "\r\n",
        "    trial_episodes = []\r\n",
        "    for trial in range(10):\r\n",
        "        env = gym.make('LunarLander-v2')\r\n",
        "\r\n",
        "        params = {\r\n",
        "            'gamma': 0.995, #0.995\r\n",
        "            'epsi_high': 0.5,   #0.7, 0.5\r\n",
        "            'epsi_low': 0.00,   #0.01\r\n",
        "            'decay': 0.995,  #0.99, 0.995\r\n",
        "            'lr': 0.0001,   #0.0001\r\n",
        "            'capacity': 400000, #100000\r\n",
        "            'batch_size': 128,  #64\r\n",
        "            'error_batch_size': 128,   #64  rename: per_bathc_size\r\n",
        "            'update_interval': 1,   # unit: episode\r\n",
        "            'state_space_dim': env.observation_space.shape[0],\r\n",
        "            'action_space_dim': env.action_space.n\r\n",
        "        }\r\n",
        "        agent = Agent(**params)\r\n",
        "\r\n",
        "\r\n",
        "        episode_rewards = []\r\n",
        "        for episode in range(1000):\r\n",
        "            s0 = env.reset()\r\n",
        "            tot_rewards = 0\r\n",
        "\r\n",
        "            while True:\r\n",
        "                # env.render()\r\n",
        "                a0 = agent.act(s0)\r\n",
        "                s1, r1, done, _ = env.step(a0)      \r\n",
        "        \r\n",
        "                agent.put(s0, a0, r1, s1, done)\r\n",
        "                tot_rewards += r1 \r\n",
        "                \r\n",
        "                if done:\r\n",
        "                    episode_rewards.append(tot_rewards)\r\n",
        "                    success = plot_rewards()\r\n",
        "                    break\r\n",
        "\r\n",
        "                s0 = s1\r\n",
        "                agent.learn()\r\n",
        "\r\n",
        "            agent.update_episode(episode_rewards)\r\n",
        "\r\n",
        "            print(\"Episode: {} Rewards: {}\".format(episode, tot_rewards))\r\n",
        "            if success:\r\n",
        "                print(\"Solved!\")\r\n",
        "                break\r\n",
        "        trial_episodes.append(episode)\r\n",
        "        print(trial_episodes)\r\n",
        "    env.close()\r\n",
        "    plt.ioff()\r\n",
        "    plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMdnpWHV_Cw6"
      },
      "source": [
        "import numpy as np \r\n",
        "print(trial_episodes)\r\n",
        "print(np.mean(trial_episodes))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}