{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_uE9Joqe-c"
      },
      "source": [
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUEHmWsnqTn4"
      },
      "source": [
        "pip install box2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzEykXkv_B-H"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        return q\n",
        "    \n",
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action):\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "            \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                    \n",
        "                \n",
        "    def save(self, directory, name):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, name))\n",
        "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target.pth' % (directory, name))\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1.pth' % (directory, name))\n",
        "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target.pth' % (directory, name))\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2.pth' % (directory, name))\n",
        "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target.pth' % (directory, name))\n",
        "        \n",
        "    def load(self, directory, name):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load('%s/%s_crtic_2.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "    def load_actor(self, directory, name):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQCzbuw8qvro"
      },
      "source": [
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "# from TD3 import TD3\n",
        "# from utils import ReplayBuffer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=5e5):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "\n",
        "\n",
        "def plot_rewards(episode_rewards):\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    rewards_t = torch.FloatTensor(episode_rewards)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Rewards')\n",
        "    plt.ylim([-500,300])\n",
        "    plt.plot(rewards_t.numpy())\n",
        "    # take 100 episode averages and plot them too\n",
        "    if len(rewards_t) >= 100:\n",
        "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.cat(99*[torch.tensor([-200])]), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if len(rewards_t) >= 100 and means[-1:] >= 200:\n",
        "        plt.title('Solved!')\n",
        "        plt.pause(0.001)\n",
        "        return True\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    num_episodes = []\n",
        "    for trial in range(10):\n",
        "        ######### Hyperparameters #########\n",
        "        env_name = \"LunarLanderContinuous-v2\"\n",
        "        log_interval = 100          # print avg reward after interval\n",
        "        random_seed = 0\n",
        "        gamma = 0.995               # discount for future rewards\n",
        "        batch_size = 128            # num of transitions sampled from replay buffer\n",
        "        lr = 0.001\n",
        "        exploration_noise = 0.2 \n",
        "        polyak = 0.995              # target policy update parameter (1-tau)\n",
        "        policy_noise = 0.2          # target policy smoothing noise\n",
        "        noise_clip = 0.5\n",
        "        policy_delay = 1            # delayed policy updates parameter\n",
        "        max_episodes = 1000         # max num of episodes\n",
        "        max_timesteps = 2000        # max timesteps in one episode\n",
        "        directory = \"./preTrained/{}\".format(env_name) # save trained models\n",
        "        filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
        "        ###################################\n",
        "        \n",
        "        env = gym.make(env_name)\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        max_action = float(env.action_space.high[0])\n",
        "        \n",
        "        policy = TD3(lr, state_dim, action_dim, max_action)\n",
        "        replay_buffer = ReplayBuffer()\n",
        "        \n",
        "        if random_seed:\n",
        "            print(\"Random Seed: {}\".format(random_seed))\n",
        "            env.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "            np.random.seed(random_seed)\n",
        "        \n",
        "        # logging variables:\n",
        "        ep_reward = 0\n",
        "        log_f = open(\"log.txt\",\"w+\")\n",
        "        \n",
        "        episode_rewards = []\n",
        "        # training procedure:\n",
        "        for episode in range(1, max_episodes+1):\n",
        "            state = env.reset()\n",
        "            for t in range(max_timesteps):\n",
        "                # select action and add exploration noise:\n",
        "                action = policy.select_action(state)\n",
        "                action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "                action = action.clip(env.action_space.low, env.action_space.high)\n",
        "                \n",
        "                # take action in env:\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "                state = next_state\n",
        "\n",
        "                ep_reward += reward\n",
        "\n",
        "                # if episode is done then update policy:\n",
        "                if done or t==(max_timesteps-1):\n",
        "                    policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
        "                    episode_rewards.append(ep_reward)\n",
        "                    success = plot_rewards(episode_rewards)\n",
        "                    break\n",
        "\n",
        "            #--- uncomment for adaptive batch size ---#\n",
        "            temp_mean = np.mean(episode_rewards[-50:])\n",
        "            if len(episode_rewards) >= 50 and temp_mean > 170:\n",
        "                batch_size = min(256, batch_size+1)\n",
        "            else:\n",
        "                batch_size = min(128, batch_size)\n",
        "            #-----------------------------------------#\n",
        "\n",
        "            print(\"episode:{} \\treward:{} \\ttimestep:{} \\tbatchsize:{} \\tmean:{} \\texploration:{}\".format(episode,  ep_reward, t, batch_size, temp_mean, exploration_noise))\n",
        "\n",
        "            # logging updates:\n",
        "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "            log_f.flush()\n",
        "            ep_reward = 0\n",
        "\n",
        "            if success:\n",
        "                print(\"Solved!\")\n",
        "                name = filename + '_solved'\n",
        "                # policy.save(directory, name)\n",
        "                log_f.close()\n",
        "                break\n",
        "            \n",
        "        num_episodes.append(episode)\n",
        "        print(num_episodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}